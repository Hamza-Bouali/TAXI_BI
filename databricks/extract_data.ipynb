{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e9c2d1-0549-4309-bf4a-3d36d324e1e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-sql-connector in /local_disk0/.ephemeral_nfs/envs/pythonEnv-81c129a1-7da0-4e05-aad2-bbc58db22fe2/lib/python3.12/site-packages (4.2.2)\n",
      "Requirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/envs/pythonEnv-81c129a1-7da0-4e05-aad2-bbc58db22fe2/lib/python3.12/site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy in /databricks/python3/lib/python3.12/site-packages (2.1.3)\n",
      "Requirement already satisfied: lz4<5.0.0,>=4.0.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-81c129a1-7da0-4e05-aad2-bbc58db22fe2/lib/python3.12/site-packages (from databricks-sql-connector) (4.4.5)\n",
      "Requirement already satisfied: oauthlib<4.0.0,>=3.1.0 in /usr/lib/python3/dist-packages (from databricks-sql-connector) (3.2.2)\n",
      "Requirement already satisfied: pybreaker<2.0.0,>=1.0.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-81c129a1-7da0-4e05-aad2-bbc58db22fe2/lib/python3.12/site-packages (from databricks-sql-connector) (1.4.1)\n",
      "Requirement already satisfied: pyjwt<3.0.0,>=2.0.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sql-connector) (2.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /databricks/python3/lib/python3.12/site-packages (from databricks-sql-connector) (2.9.0.post0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.1 in /databricks/python3/lib/python3.12/site-packages (from databricks-sql-connector) (2.32.3)\n",
      "Requirement already satisfied: thrift<0.21.0,>=0.16.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-81c129a1-7da0-4e05-aad2-bbc58db22fe2/lib/python3.12/site-packages (from databricks-sql-connector) (0.20.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in /databricks/python3/lib/python3.12/site-packages (from databricks-sql-connector) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/envs/pythonEnv-81c129a1-7da0-4e05-aad2-bbc58db22fe2/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil<3.0.0,>=2.8.0->databricks-sql-connector) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.1->databricks-sql-connector) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.1->databricks-sql-connector) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.1->databricks-sql-connector) (2025.1.31)\n",
      "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install databricks-sql-connector pandas openpyxl numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add your databricks warehouse credentials here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4e2b7a-b068-439d-a865-6ea6cd504af9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 21:44:39,189 - INFO - Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "    \n",
    "os.environ['DATABRICKS_HOST'] = '*'\n",
    "os.environ['DATABRICKS_HTTP_PATH'] = '*'\n",
    "os.environ['DATABRICKS_TOKEN'] = '*'\n",
    "os.environ['DATABRICKS_CATALOG']='*'\n",
    "os.environ['DATABRICKS_SCHEMA']='*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c47c6bc6-4d63-4b87-af14-8bc7eb36713b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 21:44:41,382 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:44:41,383 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:44:41,384 - INFO - Successfully opened session 01f0dac8-6d7a-1cec-a9ca-5c23bbe5f4e8\n",
      "2025-12-16 21:44:41,385 - INFO - âœ“ Databricks connection established\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸš• Taxi Data Warehouse - Databricks Report Generator\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-16 21:44:41,446 - INFO - \n",
      "ðŸ“„ Creating Executive Overview Sheet...\n",
      "2025-12-16 21:44:41,447 - INFO - ðŸ“Š Fetching Executive KPIs...\n",
      "2025-12-16 21:44:46,744 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:44:46,744 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:44:51,146 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:44:51,147 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:44:51,232 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:44:51,233 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:44:51,386 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:44:51,387 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:44:51,501 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:44:51,502 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:44:51,503 - INFO - ðŸ“Š Fetching Executive Zone Data...\n",
      "2025-12-16 21:44:55,226 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:44:55,227 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:44:55,235 - INFO - ðŸ“„ Creating Demand Analysis Sheet...\n",
      "2025-12-16 21:44:55,236 - INFO - ðŸ“Š Fetching Demand Summary...\n",
      "2025-12-16 21:44:56,982 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:44:56,983 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:44:56,987 - INFO - ðŸ“Š Fetching Demand Heatmap...\n",
      "2025-12-16 21:45:00,660 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:00,660 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:00,667 - INFO - ðŸ“„ Creating Revenue Analysis Sheet...\n",
      "2025-12-16 21:45:00,668 - INFO - ðŸ’° Fetching Revenue by Zone...\n",
      "2025-12-16 21:45:02,808 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:02,809 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:02,813 - INFO - ðŸ’° Fetching Revenue by Vendor...\n",
      "2025-12-16 21:45:05,702 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:05,702 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:05,709 - INFO - ðŸ“„ Creating Efficiency Sheet...\n",
      "2025-12-16 21:45:05,710 - INFO - âš¡ Fetching Efficiency Metrics...\n",
      "2025-12-16 21:45:06,710 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:06,711 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:06,715 - INFO - âš¡ Fetching Efficiency by Zone...\n",
      "2025-12-16 21:45:07,929 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:07,931 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:07,940 - INFO - ðŸ“„ Creating Spatial Flow Sheet...\n",
      "2025-12-16 21:45:07,942 - INFO - ðŸ—ºï¸  Fetching Spatial Flow Data...\n",
      "2025-12-16 21:45:10,731 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:10,731 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:10,737 - INFO - ðŸ“„ Creating Passenger Behavior Sheet...\n",
      "2025-12-16 21:45:10,738 - INFO - ðŸ‘¥ Fetching Passenger Distribution...\n",
      "2025-12-16 21:45:12,904 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:12,905 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:12,911 - INFO - ðŸ“„ Creating Payment & Pricing Sheet...\n",
      "2025-12-16 21:45:12,911 - INFO - ðŸ’³ Fetching Payment Analysis...\n",
      "2025-12-16 21:45:15,130 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:15,131 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:15,135 - INFO - ðŸ’³ Fetching Rate Code Analysis...\n",
      "2025-12-16 21:45:17,438 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:17,439 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:17,452 - INFO - ðŸ“„ Creating Vendor Performance Sheet...\n",
      "2025-12-16 21:45:17,453 - INFO - ðŸš• Fetching Vendor Performance...\n",
      "2025-12-16 21:45:19,578 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:19,578 - INFO - HTTP Response with status code 200, message: OK\n",
      "2025-12-16 21:45:19,608 - INFO - \n",
      "ðŸŽ¨ Applying formatting...\n",
      "2025-12-16 21:45:19,848 - INFO - \n",
      "âœ“ Report generated successfully: Taxi_Report_20251216_214441.xlsx\n",
      "2025-12-16 21:45:19,849 - INFO - Closing session 01f0dac8-6d7a-1cec-a9ca-5c23bbe5f4e8\n",
      "2025-12-16 21:45:19,904 - INFO - Received status code 200 for POST request\n",
      "2025-12-16 21:45:19,905 - INFO - HTTP Response with status code 200, message: OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ“ Report saved as: Taxi_Report_20251216_214441.xlsx\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# The code is mostly correct, but replace !pip with %pip for Databricks compatibility.\n",
    "from datetime import datetime\n",
    "from databricks import sql\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils import get_column_letter\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TaxiReportGenerator:\n",
    "    def __init__(self, server_hostname, http_path, access_token, catalog, schema):\n",
    "        \"\"\"\n",
    "        Initialize Databricks connection\n",
    "        \n",
    "        Args:\n",
    "            server_hostname: Databricks workspace URL (e.g., 'adb-xxxxxxx.azuredatabricks.net')\n",
    "            http_path: SQL warehouse HTTP path (e.g., '/sql/1.0/warehouses/xxxxxxx')\n",
    "            access_token: Personal access token or service principal token\n",
    "            catalog: Catalog name (e.g., 'main')\n",
    "            schema: Schema name (e.g., 'taxi_warehouse')\n",
    "        \"\"\"\n",
    "        self.server_hostname = server_hostname\n",
    "        self.http_path = http_path\n",
    "        self.access_token = access_token\n",
    "        self.catalog = catalog\n",
    "        self.schema = schema\n",
    "        self.conn = None\n",
    "        self.output_file = f'Taxi_Report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.xlsx'\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"Establish Databricks connection\"\"\"\n",
    "        try:\n",
    "            self.conn = sql.connect(\n",
    "                server_hostname=self.server_hostname,\n",
    "                http_path=self.http_path,\n",
    "                access_token=self.access_token\n",
    "            )\n",
    "            logger.info(\"âœ“ Databricks connection established\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âœ— Connection failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def query(self, sql_query):\n",
    "        \"\"\"Execute SQL query and return DataFrame\"\"\"\n",
    "        try:\n",
    "            cursor = self.conn.cursor()\n",
    "            cursor.execute(sql_query)\n",
    "            result = cursor.fetchall()\n",
    "            \n",
    "            # Get column names\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(result, columns=columns)\n",
    "            cursor.close()\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âœ— Query failed: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_executive_kpis(self):\n",
    "        \"\"\"Get executive overview KPIs\"\"\"\n",
    "        logger.info(\"ðŸ“Š Fetching Executive KPIs...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT trip_id) as total_trips,\n",
    "            ROUND(SUM(fare_amount), 2) as total_revenue,\n",
    "            SUM(passenger_count) as total_passengers,\n",
    "            ROUND(AVG(trip_duration_minutes), 2) as avg_duration,\n",
    "            ROUND(AVG(trip_distance_miles), 2) as avg_distance,\n",
    "            ROUND(AVG(fare_amount), 2) as avg_fare_per_trip,\n",
    "            ROUND(SUM(fare_amount) / COUNT(DISTINCT trip_id), 2) as revenue_per_trip\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_executive_zones(self):\n",
    "        \"\"\"Get top zones by trip count and revenue\"\"\"\n",
    "        logger.info(\"ðŸ“Š Fetching Executive Zone Data...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            z.zone_name,\n",
    "            z.borough,\n",
    "            COUNT(DISTINCT ft.trip_id) as trips,\n",
    "            ROUND(SUM(ft.fare_amount), 2) as revenue,\n",
    "            ROUND(COUNT(DISTINCT ft.trip_id) * 100.0 / \n",
    "                  (SELECT COUNT(*) FROM {self.catalog}.{self.schema}.fact_trips), 2) as pct_total\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips ft\n",
    "        JOIN {self.catalog}.{self.schema}.dim_location z ON ft.pickup_location_key = z.location_key\n",
    "        GROUP BY z.zone_name, z.borough\n",
    "        ORDER BY trips DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_demand_summary(self):\n",
    "        \"\"\"Get demand analysis summary\"\"\"\n",
    "        logger.info(\"ðŸ“Š Fetching Demand Summary...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            COUNT(DISTINCT trip_id) as total_trips,\n",
    "            SUM(passenger_count) as total_passengers,\n",
    "            ROUND(AVG(CAST(passenger_count AS FLOAT)), 2) as avg_passengers_per_trip\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_demand_heatmap(self):\n",
    "        \"\"\"Get demand heatmap by day of week and hour\"\"\"\n",
    "        logger.info(\"ðŸ“Š Fetching Demand Heatmap...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            dd.day_name,\n",
    "            HOUR(from_unixtime(cast(dd.date_key as bigint) / 1000)) as hour,\n",
    "            COUNT(DISTINCT ft.trip_id) as trip_count\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips ft\n",
    "        JOIN {self.catalog}.{self.schema}.dim_date dd ON ft.pickup_date_key = dd.date_key\n",
    "        GROUP BY dd.day_name, HOUR(from_unixtime(cast(dd.date_key as bigint) / 1000))\n",
    "        ORDER BY \n",
    "            CASE dd.day_name \n",
    "                WHEN 'Monday' THEN 1\n",
    "                WHEN 'Tuesday' THEN 2\n",
    "                WHEN 'Wednesday' THEN 3\n",
    "                WHEN 'Thursday' THEN 4\n",
    "                WHEN 'Friday' THEN 5\n",
    "                WHEN 'Saturday' THEN 6\n",
    "                WHEN 'Sunday' THEN 7\n",
    "            END, hour\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_revenue_by_zone(self):\n",
    "        \"\"\"Get revenue analysis by zone\"\"\"\n",
    "        logger.info(\"ðŸ’° Fetching Revenue by Zone...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            z.zone_name,\n",
    "            COUNT(DISTINCT ft.trip_id) as trip_count,\n",
    "            ROUND(SUM(ft.fare_amount), 2) as total_revenue,\n",
    "            ROUND(AVG(ft.fare_amount), 2) as avg_fare,\n",
    "            ROUND(SUM(ft.fare_amount) / SUM(ft.trip_distance_miles), 2) as revenue_per_km\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips ft\n",
    "        JOIN {self.catalog}.{self.schema}.dim_location z ON ft.pickup_location_key = z.location_key\n",
    "        GROUP BY z.zone_name\n",
    "        ORDER BY total_revenue DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_revenue_by_vendor(self):\n",
    "        \"\"\"Get revenue analysis by vendor\"\"\"\n",
    "        logger.info(\"ðŸ’° Fetching Revenue by Vendor...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            v.vendor_name,\n",
    "            COUNT(DISTINCT ft.trip_id) as trips,\n",
    "            ROUND(SUM(ft.fare_amount), 2) as total_revenue,\n",
    "            ROUND(AVG(ft.fare_amount), 2) as avg_fare,\n",
    "            ROUND(SUM(ft.fare_amount) / COUNT(DISTINCT ft.trip_id), 2) as revenue_per_trip\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips ft\n",
    "        JOIN {self.catalog}.{self.schema}.dim_vendor v ON ft.vendor_key = v.vendor_key\n",
    "        GROUP BY v.vendor_name\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_efficiency_summary(self):\n",
    "        \"\"\"Get trip efficiency metrics\"\"\"\n",
    "        logger.info(\"âš¡ Fetching Efficiency Metrics...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            ROUND(AVG(trip_duration_minutes), 2) as avg_duration_min,\n",
    "            ROUND(AVG(trip_distance_miles), 2) as avg_distance_miles,\n",
    "            ROUND(AVG(trip_distance_miles) / NULLIF(AVG(trip_duration_minutes), 0) * 60, 2) as avg_speed_kmh\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_efficiency_by_zone(self):\n",
    "        \"\"\"Get efficiency metrics by zone\"\"\"\n",
    "        logger.info(\"âš¡ Fetching Efficiency by Zone...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            z.zone_name,\n",
    "            ROUND(AVG(ft.trip_duration_minutes), 2) as avg_duration,\n",
    "            ROUND(AVG(ft.trip_distance_miles), 2) as avg_distance,\n",
    "            ROUND(AVG(ft.trip_distance_miles) / NULLIF(AVG(ft.trip_duration_minutes), 0) * 60, 2) as avg_speed\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips ft\n",
    "        JOIN {self.catalog}.{self.schema}.dim_location z ON ft.pickup_location_key = z.location_key\n",
    "        GROUP BY z.zone_name\n",
    "        ORDER BY avg_speed DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_spatial_flow(self):\n",
    "        \"\"\"Get top routes (pickup â†’ dropoff)\"\"\"\n",
    "        logger.info(\"ðŸ—ºï¸  Fetching Spatial Flow Data...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            zp.zone_name as pickup_zone,\n",
    "            zd.zone_name as dropoff_zone,\n",
    "            COUNT(DISTINCT ft.trip_id) as trip_count,\n",
    "            ROUND(AVG(ft.trip_duration_minutes), 2) as avg_duration,\n",
    "            ROUND(AVG(ft.trip_distance_miles), 2) as avg_distance,\n",
    "            ROUND(SUM(ft.fare_amount), 2) as total_revenue\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips ft\n",
    "        JOIN {self.catalog}.{self.schema}.dim_location zp ON ft.pickup_location_key = zp.location_key\n",
    "        JOIN {self.catalog}.{self.schema}.dim_location zd ON ft.dropoff_location_key = zd.location_key\n",
    "        GROUP BY zp.zone_name, zd.zone_name\n",
    "        ORDER BY trip_count DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_passenger_distribution(self):\n",
    "        \"\"\"Get passenger count distribution\"\"\"\n",
    "        logger.info(\"ðŸ‘¥ Fetching Passenger Distribution...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            passenger_count,\n",
    "            COUNT(DISTINCT trip_id) as trip_count,\n",
    "            ROUND(COUNT(DISTINCT trip_id) * 100.0 / \n",
    "                  (SELECT COUNT(*) FROM {self.catalog}.{self.schema}.fact_trips), 2) as pct_total,\n",
    "            ROUND(AVG(fare_amount), 2) as avg_revenue\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips\n",
    "        GROUP BY passenger_count\n",
    "        ORDER BY passenger_count\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_payment_analysis(self):\n",
    "        \"\"\"Get payment type analysis\"\"\"\n",
    "        logger.info(\"ðŸ’³ Fetching Payment Analysis...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            p.payment_name,\n",
    "            COUNT(DISTINCT ft.trip_id) as trips,\n",
    "            ROUND(SUM(ft.fare_amount), 2) as total_revenue,\n",
    "            ROUND(AVG(ft.fare_amount), 2) as avg_fare,\n",
    "            p.is_electronic\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips ft\n",
    "        JOIN {self.catalog}.{self.schema}.dim_payment_type p ON ft.payment_type_key = p.payment_type_key\n",
    "        GROUP BY p.payment_name, p.is_electronic\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_payment_by_rate(self):\n",
    "        \"\"\"Get analysis by rate code\"\"\"\n",
    "        logger.info(\"ðŸ’³ Fetching Rate Code Analysis...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            r.rate_code_desc,\n",
    "            COUNT(DISTINCT ft.trip_id) as trips,\n",
    "            ROUND(SUM(ft.fare_amount), 2) as total_revenue,\n",
    "            ROUND(AVG(ft.fare_amount), 2) as avg_fare\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips ft\n",
    "        JOIN {self.catalog}.{self.schema}.dim_rate_code r ON ft.rate_code_key = r.rate_code_key\n",
    "        GROUP BY r.rate_code_desc\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def get_vendor_performance(self):\n",
    "        \"\"\"Get comprehensive vendor performance\"\"\"\n",
    "        logger.info(\"ðŸš• Fetching Vendor Performance...\")\n",
    "        \n",
    "        sql_query = f\"\"\"\n",
    "        SELECT \n",
    "            v.vendor_name,\n",
    "            COUNT(DISTINCT ft.trip_id) as total_trips,\n",
    "            ROUND(SUM(ft.fare_amount), 2) as total_revenue,\n",
    "            ROUND(AVG(ft.trip_duration_minutes), 2) as avg_duration,\n",
    "            ROUND(AVG(ft.trip_distance_miles), 2) as avg_distance,\n",
    "            ROUND(SUM(ft.fare_amount) / COUNT(DISTINCT ft.trip_id), 2) as revenue_per_trip,\n",
    "            SUM(ft.passenger_count) as total_passengers,\n",
    "            ROUND(AVG(CAST(ft.passenger_count AS FLOAT)), 2) as avg_passengers\n",
    "        FROM {self.catalog}.{self.schema}.fact_trips ft\n",
    "        JOIN {self.catalog}.{self.schema}.dim_vendor v ON ft.vendor_key = v.vendor_key\n",
    "        GROUP BY v.vendor_name\n",
    "        ORDER BY total_revenue DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.query(sql_query)\n",
    "    \n",
    "    def format_excel_header(self, ws, row):\n",
    "        \"\"\"Format header row with styling\"\"\"\n",
    "        header_fill = PatternFill(start_color=\"667EEA\", end_color=\"667EEA\", fill_type=\"solid\")\n",
    "        header_font = Font(color=\"FFFFFF\", bold=True, size=11)\n",
    "        border = Border(left=Side(style='thin'), right=Side(style='thin'),\n",
    "                       top=Side(style='thin'), bottom=Side(style='thin'))\n",
    "        \n",
    "        for cell in ws[row]:\n",
    "            if cell.value:\n",
    "                cell.fill = header_fill\n",
    "                cell.font = header_font\n",
    "                cell.alignment = Alignment(horizontal='center', vertical='center', wrap_text=True)\n",
    "                cell.border = border\n",
    "    \n",
    "    def format_data_rows(self, ws, start_row, end_row):\n",
    "        \"\"\"Format data rows\"\"\"\n",
    "        border = Border(left=Side(style='thin'), right=Side(style='thin'),\n",
    "                       top=Side(style='thin'), bottom=Side(style='thin'))\n",
    "        \n",
    "        for row in ws.iter_rows(min_row=start_row, max_row=end_row):\n",
    "            for cell in row:\n",
    "                cell.border = border\n",
    "                cell.alignment = Alignment(horizontal='left', vertical='center')\n",
    "                if isinstance(cell.value, (int, float)):\n",
    "                    cell.alignment = Alignment(horizontal='right', vertical='center')\n",
    "    \n",
    "    def autofit_columns(self, ws):\n",
    "        \"\"\"Auto-fit column widths\"\"\"\n",
    "        for column in ws.columns:\n",
    "            max_length = 0\n",
    "            column_letter = get_column_letter(column[0].column)\n",
    "            for cell in column:\n",
    "                try:\n",
    "                    if len(str(cell.value)) > max_length:\n",
    "                        max_length = len(str(cell.value))\n",
    "                except:\n",
    "                    pass\n",
    "            adjusted_width = min(max_length + 2, 50)\n",
    "            ws.column_dimensions[column_letter].width = adjusted_width\n",
    "    \n",
    "    def create_report(self):\n",
    "        \"\"\"Generate complete Excel report\"\"\"\n",
    "        if not self.connect():\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            with pd.ExcelWriter(self.output_file, engine='openpyxl') as writer:\n",
    "                # Executive Overview\n",
    "                logger.info(\"\\nðŸ“„ Creating Executive Overview Sheet...\")\n",
    "                kpis = self.get_executive_kpis()\n",
    "                zones = self.get_executive_zones()\n",
    "                \n",
    "                kpis.to_excel(writer, sheet_name='Executive KPIs', index=False, startrow=0)\n",
    "                zones.to_excel(writer, sheet_name='Executive KPIs', index=False, startrow=len(kpis) + 3)\n",
    "                \n",
    "                # Demand Analysis\n",
    "                logger.info(\"ðŸ“„ Creating Demand Analysis Sheet...\")\n",
    "                demand_summary = self.get_demand_summary()\n",
    "                demand_heatmap = self.get_demand_heatmap()\n",
    "                \n",
    "                demand_summary.to_excel(writer, sheet_name='Demand', index=False, startrow=0)\n",
    "                demand_heatmap.to_excel(writer, sheet_name='Demand', index=False, startrow=len(demand_summary) + 3)\n",
    "                \n",
    "                # Revenue Analysis\n",
    "                logger.info(\"ðŸ“„ Creating Revenue Analysis Sheet...\")\n",
    "                revenue_zone = self.get_revenue_by_zone()\n",
    "                revenue_vendor = self.get_revenue_by_vendor()\n",
    "                \n",
    "                revenue_zone.to_excel(writer, sheet_name='Revenue', index=False, startrow=0)\n",
    "                revenue_vendor.to_excel(writer, sheet_name='Revenue', index=False, startrow=len(revenue_zone) + 3)\n",
    "                \n",
    "                # Trip Efficiency\n",
    "                logger.info(\"ðŸ“„ Creating Efficiency Sheet...\")\n",
    "                efficiency_summary = self.get_efficiency_summary()\n",
    "                efficiency_zone = self.get_efficiency_by_zone()\n",
    "                \n",
    "                efficiency_summary.to_excel(writer, sheet_name='Efficiency', index=False, startrow=0)\n",
    "                efficiency_zone.to_excel(writer, sheet_name='Efficiency', index=False, startrow=3)\n",
    "                \n",
    "                # Spatial Flow\n",
    "                logger.info(\"ðŸ“„ Creating Spatial Flow Sheet...\")\n",
    "                spatial = self.get_spatial_flow()\n",
    "                spatial.to_excel(writer, sheet_name='Spatial Flow', index=False)\n",
    "                \n",
    "                # Passenger Behavior\n",
    "                logger.info(\"ðŸ“„ Creating Passenger Behavior Sheet...\")\n",
    "                passengers = self.get_passenger_distribution()\n",
    "                passengers.to_excel(writer, sheet_name='Passenger Behavior', index=False)\n",
    "                \n",
    "                # Payment & Pricing\n",
    "                logger.info(\"ðŸ“„ Creating Payment & Pricing Sheet...\")\n",
    "                payment = self.get_payment_analysis()\n",
    "                payment_rate = self.get_payment_by_rate()\n",
    "                \n",
    "                payment.to_excel(writer, sheet_name='Payment', index=False, startrow=0)\n",
    "                payment_rate.to_excel(writer, sheet_name='Payment', index=False, startrow=len(payment) + 3)\n",
    "                \n",
    "                # Vendor Performance\n",
    "                logger.info(\"ðŸ“„ Creating Vendor Performance Sheet...\")\n",
    "                vendor = self.get_vendor_performance()\n",
    "                vendor.to_excel(writer, sheet_name='Vendor Performance', index=False)\n",
    "            \n",
    "            # Apply formatting\n",
    "            logger.info(\"\\nðŸŽ¨ Applying formatting...\")\n",
    "            self.apply_formatting()\n",
    "            \n",
    "            logger.info(f\"\\nâœ“ Report generated successfully: {self.output_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"\\nâœ— Report generation failed: {e}\")\n",
    "            return False\n",
    "        finally:\n",
    "            if self.conn:\n",
    "                self.conn.close()\n",
    "    \n",
    "    def apply_formatting(self):\n",
    "        \"\"\"Apply formatting to all sheets\"\"\"\n",
    "        wb = load_workbook(self.output_file)\n",
    "        \n",
    "        for sheet_name in wb.sheetnames:\n",
    "            ws = wb[sheet_name]\n",
    "            \n",
    "            # Format header (first row)\n",
    "            self.format_excel_header(ws, 1)\n",
    "            \n",
    "            # Format data rows\n",
    "            if ws.max_row > 1:\n",
    "                self.format_data_rows(ws, 2, ws.max_row)\n",
    "            \n",
    "            # Auto-fit columns\n",
    "            self.autofit_columns(ws)\n",
    "        \n",
    "        wb.save(self.output_file)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸš• Taxi Data Warehouse - Databricks Report Generator\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Databricks connection parameters\n",
    "    # You can also load these from environment variables or config file\n",
    "    SERVER_HOSTNAME = os.getenv('DATABRICKS_HOST', 'your-workspace.azuredatabricks.net')\n",
    "    HTTP_PATH = os.getenv('DATABRICKS_HTTP_PATH', '/sql/1.0/warehouses/your-warehouse-id')\n",
    "    ACCESS_TOKEN = os.getenv('DATABRICKS_TOKEN', 'your-access-token')\n",
    "    CATALOG = os.getenv('DATABRICKS_CATALOG', 'main')\n",
    "    SCHEMA = os.getenv('DATABRICKS_SCHEMA', 'taxi_warehouse')\n",
    "    \n",
    "    # Create generator and generate report\n",
    "    generator = TaxiReportGenerator(\n",
    "        server_hostname=SERVER_HOSTNAME,\n",
    "        http_path=HTTP_PATH,\n",
    "        access_token=ACCESS_TOKEN,\n",
    "        catalog=CATALOG,\n",
    "        schema=SCHEMA\n",
    "    )\n",
    "    \n",
    "    if generator.create_report():\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"âœ“ Report saved as: {generator.output_file}\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"âœ— Failed to generate report\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "extract_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

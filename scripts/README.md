# TAX_BI Scripts

Utility scripts for managing the NYC Green Taxi data pipeline.

## Clean Test Data

### Description
Removes all test data from both staging and data warehouse databases to prepare for production backfills.

### What It Does
1. **Staging Database**: Drops all `green_taxi_*` tables
2. **Data Warehouse**: Truncates the `fact_trips` table (preserves schema and dimensions)

### Usage

**Option 1: Run via bash wrapper (recommended)**
```bash
./scripts/clean_test_data.sh
```

**Option 2: Run Python script directly in container**
```bash
docker compose exec -T airflow-worker python - < scripts/clean_test_data.py
```

**Option 3: Interactive mode with confirmation**
```bash
docker compose exec airflow-worker python /opt/airflow/scripts/clean_test_data.py
```

### Output Example
```
============================================================
NYC GREEN TAXI - TEST DATA CLEANUP SCRIPT
============================================================

This script will:
  1. Drop all green_taxi_* tables from staging database
  2. Truncate fact_trips table in data warehouse

âš ï¸  WARNING: This action cannot be undone!

============================================================
CLEANING STAGING DATABASE
============================================================
Found 6 staging tables:
  â€¢ green_taxi_2024_01
  â€¢ green_taxi_2024_02
  ...

Dropping tables...
  âœ“ Dropped: green_taxi_2024_01
  âœ“ Dropped: green_taxi_2024_02
  ...

âœ… Successfully dropped 6 staging tables

============================================================
CLEANING DATA WAREHOUSE
============================================================
Records before cleanup: 637,801
Date range: 20240101 â†’ 20251031

Truncating fact_trips table...
Records after cleanup: 0

âœ… Successfully removed 637,801 records from data warehouse

============================================================
CLEANUP SUMMARY
============================================================
Staging Database: âœ… SUCCESS
Data Warehouse:   âœ… SUCCESS
============================================================

ðŸŽ‰ All test data cleaned successfully!
```

### After Cleanup

Run a fresh backfill:
```bash
docker compose exec airflow-scheduler airflow backfill create \
  --dag-id nyc_green_taxi_pipeline \
  --from-date 2024-01-01 \
  --to-date 2025-10-31 \
  --max-active-runs 3
```

### Safety Features
- âœ… Interactive confirmation prompt (when run in TTY)
- âœ… Transaction-based operations (rollback on error)
- âœ… Detailed logging of all actions
- âœ… Pre-cleanup statistics displayed
- âœ… Post-cleanup verification

### Credentials
The script uses credentials embedded in `/dags/etl_pipeline.py`:
- **Staging**: Supabase (EU Central 2)
- **Data Warehouse**: Supabase (EU North 1)

### Notes
- âš ï¸ Does NOT drop dimension tables (dim_date, dim_location, etc.)
- âš ï¸ Does NOT affect Airflow metadata
- âœ… Preserves data warehouse schema structure
- âœ… Safe to run multiple times (idempotent)
